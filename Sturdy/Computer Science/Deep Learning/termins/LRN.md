Local Response Normalization (LRN) was first introduced in AlexNet architecture where the activation function used was ReLU as opposed to the more common tanh and sigmoid at that time.

- LRN was firrst introduced in the AlexNet architecture.
- It aims to encourage lateral inhibition, mimicking the behavior of a biological neuron.
- Helps normalize the input, making the model less sensitive to the scale of input.
![[Screen Shot 2023-09-27 at 09.00.55.png]]

