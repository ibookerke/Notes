In mathematics, gradient descent (also often called steepest descent) is **a first-order iterative optimization algorithm for finding a local minimum of a differentiable function**.

Steepest descent - basically just chossing a fixed step size η([[Learning rate]])