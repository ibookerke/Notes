**Tanh Activation** is an [[Activation function]] used for neural networks:

![[Pasted image 20230927033621.png]]
![[Screen Shot 2023-09-27 at 03.37.54.png]]

Historically, the tanh function became preferred over the [[Sigmoid]] as it gave better performance for multi-layer neural networks. But it did not solve the vanishing gradient problem that sigmoids suffered, which was tackled more effectively with the introduction of [[ReLU]] activations.

