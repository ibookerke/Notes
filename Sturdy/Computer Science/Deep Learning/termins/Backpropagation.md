Backpropagation, short for "backward propagation of errors," isÂ **an algorithm for supervised learning of artificial neural networks using [[Gradient descent]]. Given an artificial neural network and an [[Error Function]], the method calculates the gradient(derivative) of the [[error function]] with respect to the neural network's weights.

More in [[Part 8 - Multilayer Perceptrons]]

Explanation:
https://www.youtube.com/playlist?list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1
