A **rectified linear unit** (ReLU) is an [[Activation function]] that introduces the property of non-linearity to a deep learning model and solves the vanishing gradients issue. "It interprets the positive part of its argument. It is one of the most popular activation functions in deep learning


![[Pasted image 20230927033447.png]]
