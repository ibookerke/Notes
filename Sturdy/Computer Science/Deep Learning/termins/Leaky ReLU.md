Leaky ReLU isÂ **an activation function used in artificial neural networks to introduce nonlinearity among the outputs between layers of a neural network**. This activation function was created to solve the dying ReLU problem using the standard ReLU function that makes the neural network die during training.

![[Pasted image 20230927081931.png]]